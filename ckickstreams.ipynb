{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import findspark\n",
    "findspark.init('C:\\Spark')\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memoryOverhead', '4g'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.driver.memoryOverhead', '4g'),\n",
       " ('spark.app.id', 'local-1679495691045'),\n",
       " ('spark.local.dir', '../../spark_local_dir'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.app.submitTime', '1679495689007'),\n",
       " ('spark.driver.host', '10.16.98.48'),\n",
       " ('spark.executor.memory', '16g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.sql.shuffle.partitions', '200'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PysparkDataPreprocessor'),\n",
       " ('spark.cores.max', '24'),\n",
       " ('spark.app.startTime', '1679495689257'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.driver.port', '49213')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './main_data/'\n",
    "\n",
    "spark_conf = pyspark.SparkConf()\n",
    "spark_conf.setMaster(\"local[*]\").setAppName(\"PysparkDataPreprocessor\")\n",
    "spark_conf.set(\"spark.driver.maxResultSize\", \"4g\")\n",
    "spark_conf.set(\"spark.executor.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.executor.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.driver.memory\", \"16g\")\n",
    "spark_conf.set(\"spark.driver.memoryOverhead\", \"4g\")\n",
    "spark_conf.set(\"spark.cores.max\", \"24\")\n",
    "spark_conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark_conf.set(\"spark.local.dir\", \"../../spark_local_dir\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'clickstream.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------------+\n",
      "|             user_id|cat_id|          timestamp|\n",
      "+--------------------+------+-------------------+\n",
      "|000143baebad4467a...|   165|2021-01-30 20:08:12|\n",
      "|000143baebad4467a...|   165|2021-01-31 20:06:29|\n",
      "+--------------------+------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_data = source_data.drop('new_uid')\n",
    "source_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.preprocessing import PysparkDataPreprocessor\n",
    "\n",
    "preprocessor = PysparkDataPreprocessor(\n",
    "    col_id='user_id',\n",
    "    col_event_time='timestamp',\n",
    "    event_time_transformation='dt_to_timestamp',\n",
    "    cols_category=['cat_id'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 93.8 ms\n",
      "Wall time: 2min 50s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19623"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_pysparkdf = preprocessor.fit_transform(source_data).persist()\n",
    "dataset_pysparkdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|             user_id|              cat_id|          event_time|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|018d951f1ccb4f288...|[29, 1, 1, 60, 60...|[1611903458, 1612...|\n",
      "|01ef0e619d0f4a6eb...|[4, 2, 2, 3, 23, ...|[1610973443, 1612...|\n",
      "|0570e0db5cbb47b1a...|[12, 8, 11, 36, 3...|[1612968600, 1613...|\n",
      "|0677be173662457d8...|[12, 12, 12, 12, ...|[1622604120, 1622...|\n",
      "|0914be36f20d4c9fb...|[1, 1, 9, 9, 9, 3...|[1612744599, 1612...|\n",
      "|0d8486356bb14e509...|[1, 1, 3, 3, 3, 1...|[1613341920, 1613...|\n",
      "|0ee126d04de04082b...|[6, 5, 5, 6, 49, ...|[1612281145, 1612...|\n",
      "|1327d47e0b9f43c6b...|[7, 10, 1, 22, 3,...|[1612679673, 1612...|\n",
      "|13ccb15a7ba046f4a...|[2, 23, 23, 2, 23...|[1611412404, 1611...|\n",
      "|1b0cf53c7ef342159...|[3, 3, 1, 1, 1, 1...|[1613761740, 1613...|\n",
      "|1e42703fbc9945949...|[10, 12, 1, 10, 1...|[1611207638, 1611...|\n",
      "|201b130b924f4e31b...|[134, 4, 8, 8, 12...|[1611232125, 1611...|\n",
      "|20ffea50cd5e4ee1b...|[3, 2, 2, 2, 3, 4...|[1611410422, 1611...|\n",
      "|2382fc75e85e455e9...|[6, 21, 6, 6, 21,...|[1610935270, 1611...|\n",
      "|2921f36f68484f13b...|[6, 24, 6, 3, 3, ...|[1612119600, 1612...|\n",
      "|2d6bd0c3242949fa9...|[3, 3, 25, 38, 3,...|[1611509790, 1611...|\n",
      "|2de32725df3243598...|[9, 26, 26, 26, 2...|[1611015389, 1611...|\n",
      "|3119e4abab604b4e9...|[41, 38, 38, 38, ...|[1612174440, 1612...|\n",
      "|31a55cded02049f6b...|[68, 3, 4, 68, 4,...|[1610899375, 1610...|\n",
      "|325e5ae84e4c41238...|[6, 6, 3, 3, 6, 6...|[1611730501, 1612...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_pysparkdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat_id': 404}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.get_category_dictionary_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('preprocessor.p', 'wb') as f:\n",
    "    pickle.dump(preprocessor, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'train.csv'))\n",
    "matching = spark.read.options(header=True, inferSchema=True).csv(os.path.join(data_path, 'train_matching.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                bank|                 rtk|\n",
      "+--------------------+--------------------+\n",
      "|178b387813ac4a63a...|e19c0f141e9442c5b...|\n",
      "|47cffa46e6b04389b...|7df3371aabd349e4a...|\n",
      "|f73b767cfd72472aa...|b23d62c7e41145a7a...|\n",
      "|48da649603734185b...|63ad789541c54463a...|\n",
      "|37304ef19de542ee8...|c0e96de5dd594d948...|\n",
      "|3c26cb845a4941ca9...|647e736a6a064cb5b...|\n",
      "|a4840524c1b64416a...|b29febc9938749b59...|\n",
      "|9a5e1c28552f4b82b...|41812d47e3614cc18...|\n",
      "|d9968143901d4914a...|ec1adc1e08c7403fa...|\n",
      "|22bef074cccb4a2a9...|9bf3775d68644f718...|\n",
      "|31ef5cb8b89b4a16a...|6de89fa7fd054d9a9...|\n",
      "|611fad24d9a44be1b...|a66eca3f820149d29...|\n",
      "|09910580d99e44c28...|121e22e062b84acea...|\n",
      "|b8e64a76a78a4036a...|fe433f40eb194c89b...|\n",
      "|33af0b2b21ec4227a...|a24756560ecc44c2a...|\n",
      "|5047955ee2654efaa...|aa79c504266a4d1a8...|\n",
      "|fdc09565cf544c30b...|bf962314ef3040d5a...|\n",
      "|0ca3e196bc3942e89...|238029b5fcd242359...|\n",
      "|3d08d56ee08944799...|0f08048f4ff84e37b...|\n",
      "|1d9acce9245d489db...|661166af461845f9b...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matching.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|                bank|higher_education|\n",
      "+--------------------+----------------+\n",
      "|3755b59782464456b...|             0.0|\n",
      "|604a550439d644718...|             0.0|\n",
      "|542d4776ebe5454fb...|             1.0|\n",
      "|ee37fecea44d475ca...|             0.0|\n",
      "|18617a1100f44a99b...|             0.0|\n",
      "|079f07153c0149d19...|             0.0|\n",
      "|6dee55b3d7284d18b...|             1.0|\n",
      "|13fdbc4dbd394c7fb...|             1.0|\n",
      "|f94284392a064a93b...|             1.0|\n",
      "|8653cbc7d48148a6a...|             1.0|\n",
      "|b38863ecab6340c2a...|             1.0|\n",
      "|fe19bba195414500b...|             1.0|\n",
      "|0a0ae064b7d8423a8...|             0.0|\n",
      "|546e6e2e490f42d68...|             1.0|\n",
      "|499c20f1e53b4429b...|             1.0|\n",
      "|9a35b2d9f02f49e09...|             1.0|\n",
      "|5f3b2dbc151f4067b...|             0.0|\n",
      "|aaef9bc7a58e40bba...|             1.0|\n",
      "|91836eaf9d4e4975b...|             1.0|\n",
      "|c883d8fdb75a49daa...|             1.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|                bank|higher_education|\n",
      "+--------------------+----------------+\n",
      "|000932580e404dafb...|             1.0|\n",
      "|0020afcd52f54e9fa...|             1.0|\n",
      "|0034020d25da4951b...|             1.0|\n",
      "|0046da5a3d934f2db...|             1.0|\n",
      "|004b3ef36faa40f08...|             1.0|\n",
      "|0054a0388a8647d99...|             0.0|\n",
      "|0059cb4a0de44cff9...|             1.0|\n",
      "|005e2282c9ea4ddfb...|             1.0|\n",
      "|0082e5d4d8074f05b...|             0.0|\n",
      "|008607c1098d4e689...|             0.0|\n",
      "|0087c80c55924740b...|             1.0|\n",
      "|008a98f3d27e40b58...|             1.0|\n",
      "|008ca092454a4ecd9...|             1.0|\n",
      "|009fb6e432894d3a9...|             1.0|\n",
      "|00a000619ec24ee39...|             0.0|\n",
      "|00ad819ef6184f8b9...|             1.0|\n",
      "|00b64a4131744fe78...|             1.0|\n",
      "|00be95f5e4a8478e8...|             1.0|\n",
      "|00c834f3d1064f28a...|             0.0|\n",
      "|00d4a8c57ff14d0cb...|             1.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target = target.sort('bank')\n",
    "target.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                bank|                 rtk|\n",
      "+--------------------+--------------------+\n",
      "|000932580e404dafb...|97d2092878ea42678...|\n",
      "|0009e36b42cb4caeb...|beed41e945754ac5a...|\n",
      "|000b29acb6bd44f99...|bb1230b232af439e9...|\n",
      "|000c5327d99941fe9...|7d7b83b85f3f4584b...|\n",
      "|000e0d54d7c945ebb...|78e9b8a98fff4f019...|\n",
      "|0012e60b16f14da4b...|f4a70c0d6b8e4878b...|\n",
      "|001879c9110d46ed9...|a58617b4b3424468b...|\n",
      "|001c99d8cd6f409f8...|d2a0951ee0d445039...|\n",
      "|0020536c52ee4257b...|e0208c1d86824a09b...|\n",
      "|0020afcd52f54e9fa...|ae1a28a8428740e7a...|\n",
      "|00260161e7fd40369...|091c259cd60844afa...|\n",
      "|00262245fd4344b0b...|1262ca30efea49bc9...|\n",
      "|002d5bbe9a80403b8...|cdf0f9eb0bdd4f07a...|\n",
      "|0033ef60398646ff8...|                   0|\n",
      "|003812b529ad4e579...|                   0|\n",
      "|003d93fb918846ada...|cb4d0db7a2a5490cb...|\n",
      "|0041da6ae2ab46108...|                   0|\n",
      "|004231ff0d034d1e9...|                   0|\n",
      "|0046da5a3d934f2db...|03de39a7ee8d482b8...|\n",
      "|0047dbb5ef764871a...|a68ffabf07a14acdb...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matching = matching.sort('bank')\n",
    "matching.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = matching.join(target,'bank','outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+\n",
      "|                bank|                 rtk|higher_education|\n",
      "+--------------------+--------------------+----------------+\n",
      "|000932580e404dafb...|97d2092878ea42678...|             1.0|\n",
      "|0009e36b42cb4caeb...|beed41e945754ac5a...|            null|\n",
      "|000b29acb6bd44f99...|bb1230b232af439e9...|            null|\n",
      "|000c5327d99941fe9...|7d7b83b85f3f4584b...|            null|\n",
      "|000e0d54d7c945ebb...|78e9b8a98fff4f019...|            null|\n",
      "|0012e60b16f14da4b...|f4a70c0d6b8e4878b...|            null|\n",
      "|001879c9110d46ed9...|a58617b4b3424468b...|            null|\n",
      "|001c99d8cd6f409f8...|d2a0951ee0d445039...|            null|\n",
      "|0020536c52ee4257b...|e0208c1d86824a09b...|            null|\n",
      "|0020afcd52f54e9fa...|ae1a28a8428740e7a...|             1.0|\n",
      "|00260161e7fd40369...|091c259cd60844afa...|            null|\n",
      "|00262245fd4344b0b...|1262ca30efea49bc9...|            null|\n",
      "|002d5bbe9a80403b8...|cdf0f9eb0bdd4f07a...|            null|\n",
      "|0033ef60398646ff8...|                   0|            null|\n",
      "|0034020d25da4951b...|                null|             1.0|\n",
      "|003812b529ad4e579...|                   0|            null|\n",
      "|003d93fb918846ada...|cb4d0db7a2a5490cb...|            null|\n",
      "|0041da6ae2ab46108...|                   0|            null|\n",
      "|004231ff0d034d1e9...|                   0|            null|\n",
      "|0046da5a3d934f2db...|03de39a7ee8d482b8...|             1.0|\n",
      "+--------------------+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|                 rtk|higher_education|\n",
      "+--------------------+----------------+\n",
      "|97d2092878ea42678...|             1.0|\n",
      "|ae1a28a8428740e7a...|             1.0|\n",
      "|03de39a7ee8d482b8...|             1.0|\n",
      "|9c4627b2935041099...|             1.0|\n",
      "|5a9867b5c5b54819a...|             0.0|\n",
      "|d1119df4cce24574a...|             1.0|\n",
      "|51c52794be054932b...|             1.0|\n",
      "|cfdefb5f0db9496d9...|             0.0|\n",
      "|0aac66f864f149d3a...|             1.0|\n",
      "|bbefdcf97f73473f8...|             1.0|\n",
      "|cf57eefa27824b22b...|             1.0|\n",
      "|add3026385c0416ba...|             0.0|\n",
      "|e15004fd2b014295b...|             1.0|\n",
      "|182ad88293bb4ffa9...|             1.0|\n",
      "|274d03fc64f84f67a...|             1.0|\n",
      "|b1f376bb6b744f3ca...|             0.0|\n",
      "|ddfda8e42a434e28a...|             1.0|\n",
      "|04548acddc584bf39...|             1.0|\n",
      "|ff54a4a9cf6a429cb...|             1.0|\n",
      "|aa5e786ae5fe4618b...|             1.0|\n",
      "+--------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.drop('bank')\n",
    "train_dataset = train_dataset.dropna()\n",
    "train_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|rtk|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count,when,col\n",
    "\n",
    "check = train_dataset.select(count(when(col('rtk') == 0, 'rtk')).alias('rtk'))\n",
    "check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test dataset: 3908\n",
      "Size of train dataset 15715\n"
     ]
    }
   ],
   "source": [
    "test_df = dataset_pysparkdf.sample(fraction=0.2)\n",
    "train_df = dataset_pysparkdf.subtract(test_df)\n",
    "\n",
    "print('Size of test dataset:', test_df.count())\n",
    "print('Size of train dataset', train_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write.option(\"compression\",\"gzip\").parquet('test.parquet',mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.write.parquet('test.parquet', mode='overwrite')\n",
    "train_df.write.parquet('train.parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '35105d846c1e41b0bebc0515ddfc31e9',\n",
       " 'cat_id': tensor([32, 19, 37, 37, 12, 15, 15,  4,  4,  4, 11,  4, 11,  4, 11,  8, 53,  8,\n",
       "         11,  3,  1,  1, 45,  7, 53,  7,  7,  7,  7,  7,  7, 40, 40,  3,  8, 40,\n",
       "         40,  4,  1,  3,  3,  3,  1,  1,  1,  7,  3,  4,  1, 25,  1,  1, 25,  1,\n",
       "          1,  1,  1,  3,  3,  8,  1,  8, 37,  1,  8,  8,  5,  3,  8,  1,  4,  7,\n",
       "         37, 45, 45, 45, 15, 15, 45, 15, 45, 45, 32, 15, 15,  1, 45, 15, 45, 45,\n",
       "         15, 15, 15, 15, 45, 15, 45, 15, 12, 15, 15, 45, 45, 45, 45, 15, 45, 12,\n",
       "         15, 13, 15, 15, 45, 15, 45, 45, 15, 45,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "         12, 15,  3,  1, 45, 15, 15, 45, 45,  1,  1, 20, 20,  1,  3, 45,  4,  4,\n",
       "          7,  7,  7,  7,  4,  7, 49,  4,  7,  7, 49,  7,  7,  7,  4,  1,  7,  7,\n",
       "          7,  7,  7,  7,  7,  4,  7,  7,  7,  7,  4,  7,  7,  7,  7, 21,  7,  1,\n",
       "          1,  1,  1,  7,  7,  1,  7,  1,  7,  7, 14, 49, 11,  4,  4,  7,  3, 21,\n",
       "         15, 45, 45, 45, 45, 45, 45, 45, 45,  1, 45, 45, 45, 45, 45,  1,  1, 45,\n",
       "          1, 45,  1, 45, 45, 45, 15, 15,  7,  7,  7,  7,  7,  1,  4,  7,  7,  7,\n",
       "          1,  1,  1,  1,  3,  4,  4,  3,  1,  7,  4,  4,  8,  7,  4,  4,  4,  4,\n",
       "          8,  8,  4,  4,  4,  7,  7, 15,  1,  3,  2, 11,  2,  1,  3,  4,  4,  4,\n",
       "         11,  7,  1,  7,  7,  7,  1, 31,  1,  1, 45, 45, 45, 45,  3, 45, 45,  3,\n",
       "          4,  8,  8, 11,  4,  4,  4,  3,  3, 12,  8,  4,  3,  1,  4,  7,  7,  4,\n",
       "          4,  4,  4,  4,  4,  7,  4,  3,  4,  3, 21,  4,  4,  4,  1,  1,  1, 14,\n",
       "          3,  3,  1,  7,  1,  1,  7,  3,  3,  7,  7,  3,  1,  8, 45, 45, 32, 45,\n",
       "         45, 45, 45,  1,  1, 12,  4,  1, 15, 11,  4, 31, 31,  1, 31,  8, 31,  1,\n",
       "          7,  7,  8,  7,  7, 11, 11,  7, 14,  3, 14, 45, 45, 15, 45, 11,  8, 31,\n",
       "         31,  3,  3,  1,  1, 21,  8,  4,  4, 39,  4,  3, 31, 31,  1, 12, 12, 31,\n",
       "         14, 31,  8,  4,  8,  1,  1,  3,  1,  8, 45, 15, 45, 31, 21, 45,  1, 45,\n",
       "         31,  1,  4,  4,  8, 31, 31,  3,  1, 31,  4,  8, 45,  1, 31, 45, 31, 31,\n",
       "         31, 31, 31,  1,  4,  4,  4,  4,  1,  4,  4,  4,  4, 31, 40, 12,  3,  3,\n",
       "          3,  8, 11, 12, 45, 12, 12, 12, 12,  4, 46, 12,  7,  4, 12, 28,  8, 49,\n",
       "          8, 29, 24,  5,  5,  4,  1,  4,  4,  4,  4,  4, 11,  3,  4,  3,  1, 11,\n",
       "          4,  7,  7,  3, 49,  7,  7,  5,  7,  5,  8,  7,  1,  8,  7,  8,  7,  7,\n",
       "         11,  7,  3,  8, 15,  1,  3,  3,  4,  4,  8,  4, 11,  4,  4,  4,  1,  4,\n",
       "          4,  4,  4,  4,  4, 14,  1,  7,  8,  3,  4, 11,  4, 37, 37,  1,  3, 36,\n",
       "         46, 28, 36,  8, 28,  3,  3, 46, 28,  1,  8, 28, 28, 28, 28, 28, 28, 28,\n",
       "          4,  4,  3,  4, 28, 28, 28, 28,  1,  1, 28, 28, 28, 36, 28, 28, 28, 19,\n",
       "         28, 28, 28, 11,  4, 11,  4,  4,  4,  4,  4, 28,  8, 12,  8,  5,  8, 15,\n",
       "          8,  1, 28,  8,  8, 28,  8, 23,  4,  7,  7,  8,  7,  7,  7,  7, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12,  9, 37, 37, 40, 37, 37,  1, 37, 37, 40, 37,  1,\n",
       "         40, 37, 37,  1, 37,  3, 19,  3,  8, 19, 12, 12,  7,  7, 12, 12, 12, 15,\n",
       "         28, 14,  7,  7,  1,  1,  8,  7,  7, 15,  9, 45, 45, 45, 45, 15, 45, 15,\n",
       "          1,  3,  4,  3, 11, 11, 45,  3,  4, 11,  4,  1, 11,  1,  1, 31, 45,  9,\n",
       "          1,  1,  3,  1,  8,  1,  3,  3, 24, 24, 12,  8, 31, 31,  1, 31,  3,  1,\n",
       "          3,  3,  1,  1, 31, 23,  1,  8,  1, 11, 11,  1,  3, 20,  4, 11, 19,  8,\n",
       "         35,  7,  7,  7, 31, 31, 31,  8, 31, 31,  8,  1,  6, 45, 45, 45, 36, 31,\n",
       "         12,  4,  3, 31,  1,  3,  2,  4,  1, 31,  8, 31, 31, 31,  8, 31, 31, 31,\n",
       "         31,  7, 31,  1,  8,  8, 31, 31, 31,  8, 31,  1,  4,  4,  1, 31,  1, 12,\n",
       "         12,  1,  7,  7,  7, 31,  7,  7, 31, 31, 31, 31, 45,  1,  3],\n",
       "        dtype=torch.int32),\n",
       " 'event_time': tensor([1612149300, 1612258440, 1612258560, 1612262940, 1612357740, 1612373880,\n",
       "         1612374240, 1612375800, 1612377060, 1612382340, 1612405800, 1612405860,\n",
       "         1612406700, 1612408500, 1612409520, 1612411920, 1612415580, 1612427760,\n",
       "         1612489260, 1612497480, 1612497540, 1612497840, 1612546800, 1612592760,\n",
       "         1612663500, 1612829820, 1612917900, 1612919640, 1612919940, 1612921560,\n",
       "         1612926060, 1613038680, 1613039580, 1613042580, 1613043960, 1613045040,\n",
       "         1613045340, 1613045760, 1613049120, 1613233440, 1613235420, 1613236140,\n",
       "         1613236980, 1613240280, 1613243580, 1613277000, 1613279400, 1613279700,\n",
       "         1613282040, 1613285400, 1613286000, 1613288700, 1613289060, 1613290800,\n",
       "         1613291580, 1613294160, 1613297940, 1613306100, 1613310120, 1613310600,\n",
       "         1613311680, 1613327580, 1613329260, 1613345760, 1613360160, 1613361780,\n",
       "         1613388600, 1613395140, 1613396520, 1613397060, 1613398500, 1613523657,\n",
       "         1614535677, 1614844320, 1614844380, 1614846000, 1614850140, 1614851280,\n",
       "         1614873000, 1614879900, 1615180637, 1615261980, 1615262220, 1615262340,\n",
       "         1615262460, 1615262760, 1615262760, 1615262760, 1615262820, 1615263060,\n",
       "         1615263060, 1615264140, 1615264320, 1615264440, 1615265280, 1615266780,\n",
       "         1615266900, 1615266960, 1615267020, 1615267200, 1615267260, 1615267500,\n",
       "         1615268040, 1615268460, 1615268820, 1615269000, 1615269120, 1615269274,\n",
       "         1615269360, 1615269480, 1615269540, 1615269960, 1615269960, 1615270080,\n",
       "         1615270260, 1615270260, 1615270380, 1615270680, 1616029920, 1616031300,\n",
       "         1616033580, 1616036280, 1616036460, 1616036760, 1616037660, 1616037720,\n",
       "         1616080200, 1616140200, 1616259000, 1616262600, 1616264580, 1616265480,\n",
       "         1616266080, 1616267100, 1616267220, 1616295540, 1616295660, 1616300220,\n",
       "         1616300280, 1616325480, 1616337480, 1616342532, 1616460480, 1616467680,\n",
       "         1616505780, 1616510220, 1616510220, 1616510880, 1616515140, 1616515440,\n",
       "         1616518440, 1616521560, 1616550720, 1616550720, 1616551380, 1616551380,\n",
       "         1616551440, 1616551560, 1616551680, 1616551980, 1616552280, 1616552820,\n",
       "         1616553120, 1616554620, 1616555640, 1616556240, 1616556660, 1616557080,\n",
       "         1616557500, 1616557680, 1616557740, 1616557980, 1616558640, 1616558700,\n",
       "         1616558700, 1616560140, 1616561040, 1616562120, 1616562300, 1616609774,\n",
       "         1616786820, 1616830800, 1616845920, 1616845920, 1616846880, 1616848080,\n",
       "         1616851680, 1616924400, 1617237960, 1617238620, 1617240240, 1617240660,\n",
       "         1617241680, 1617244200, 1617244800, 1617246300, 1617252896, 1617254040,\n",
       "         1617277800, 1617281160, 1617284520, 1617382860, 1617382920, 1617383220,\n",
       "         1617384120, 1617385500, 1617387420, 1617387540, 1617388020, 1617388800,\n",
       "         1617389040, 1617390300, 1617390840, 1617391260, 1617391440, 1617392040,\n",
       "         1617467520, 1617641700, 1617645000, 1617646440, 1617646680, 1617646920,\n",
       "         1617648600, 1617650160, 1617670920, 1617672300, 1617672600, 1617673800,\n",
       "         1617674040, 1617675059, 1617676080, 1617676440, 1617679380, 1617769402,\n",
       "         1617889581, 1618077840, 1618080360, 1618083300, 1618104924, 1618114680,\n",
       "         1618117740, 1618118460, 1618118760, 1618121340, 1618123320, 1618123680,\n",
       "         1618126200, 1618126980, 1618127040, 1618129080, 1618130700, 1618132380,\n",
       "         1618156560, 1618161720, 1618194960, 1618200240, 1618277520, 1618280640,\n",
       "         1618282860, 1618287240, 1618287240, 1618330800, 1618331940, 1618452000,\n",
       "         1618457100, 1618459380, 1618460580, 1618462200, 1618463460, 1618464060,\n",
       "         1618495500, 1618495680, 1618497780, 1618497840, 1618500420, 1618502940,\n",
       "         1618542900, 1618544579, 1618544640, 1618562760, 1618566780, 1618567020,\n",
       "         1618568880, 1618571880, 1618574149, 1618574580, 1618575720, 1618590720,\n",
       "         1618626507, 1618723320, 1618724100, 1618728720, 1618731660, 1618732140,\n",
       "         1618734420, 1618746360, 1618747680, 1618749660, 1618752420, 1618753560,\n",
       "         1618755000, 1618776900, 1618797120, 1618797600, 1618798860, 1618799940,\n",
       "         1618802220, 1618802340, 1618802940, 1618803300, 1618804800, 1618805520,\n",
       "         1618807860, 1618808520, 1618809240, 1618832940, 1618833060, 1618836360,\n",
       "         1618839300, 1618839660, 1618840440, 1618842660, 1618843560, 1618850760,\n",
       "         1618958820, 1618959300, 1618960140, 1618960860, 1618961340, 1618962060,\n",
       "         1618964160, 1618964400, 1618964640, 1618965480, 1618965780, 1619013720,\n",
       "         1619014440, 1619107504, 1619113620, 1619114640, 1619115000, 1619118000,\n",
       "         1619120280, 1619150220, 1619151480, 1619159580, 1619168040, 1619215020,\n",
       "         1619249940, 1619252940, 1619272140, 1619366189, 1619412850, 1619470140,\n",
       "         1619470740, 1619484360, 1619491860, 1619504820, 1619506860, 1619523960,\n",
       "         1619528040, 1619529960, 1619530680, 1619533140, 1619533140, 1619535720,\n",
       "         1619536200, 1619536680, 1619536980, 1619539500, 1619542140, 1619545680,\n",
       "         1619548140, 1619548860, 1619549160, 1619577300, 1619578620, 1619580060,\n",
       "         1619588100, 1619607960, 1619609100, 1619609460, 1619610540, 1619610540,\n",
       "         1619615880, 1619619600, 1619620920, 1619624520, 1619624700, 1619626740,\n",
       "         1619648580, 1619651220, 1619651460, 1619652420, 1619653200, 1619657340,\n",
       "         1619664420, 1619681100, 1619697240, 1619704560, 1619716620, 1619803680,\n",
       "         1619805120, 1619809500, 1619835825, 1619928933, 1619962620, 1619963640,\n",
       "         1619965860, 1620014683, 1620048767, 1620159107, 1620580020, 1620580260,\n",
       "         1620600000, 1620618660, 1620620400, 1620620460, 1620624180, 1620627120,\n",
       "         1620633240, 1620633840, 1620651780, 1620654780, 1620657540, 1620660060,\n",
       "         1620661560, 1620663000, 1620668940, 1620672676, 1620726780, 1620766920,\n",
       "         1620770040, 1620784860, 1620791700, 1620798720, 1620798900, 1620801120,\n",
       "         1620801600, 1620801600, 1620801900, 1620802440, 1620803700, 1620806100,\n",
       "         1620807060, 1620810420, 1620814620, 1620818580, 1620827940, 1620833160,\n",
       "         1620834180, 1620834180, 1620835680, 1620994860, 1621006532, 1621055160,\n",
       "         1621060740, 1621089360, 1621340700, 1621501086, 1621541882, 1621622760,\n",
       "         1621649671, 1621846973, 1621951764, 1621998409, 1622092020, 1622097720,\n",
       "         1622098440, 1622098680, 1622226960, 1622227260, 1622232600, 1622257980,\n",
       "         1622259780, 1622260140, 1622260860, 1622261220, 1622261760, 1622262000,\n",
       "         1622262900, 1622263860, 1622264040, 1622266680, 1622266920, 1622267760,\n",
       "         1622268300, 1622268900, 1622269380, 1622269980, 1622270100, 1622270880,\n",
       "         1622270880, 1622271780, 1622272175, 1622272200, 1622272380, 1622272500,\n",
       "         1622272800, 1622273760, 1622273880, 1622273880, 1622274120, 1622274960,\n",
       "         1622276280, 1622276400, 1622280660, 1622281080, 1622281440, 1622290320,\n",
       "         1622401200, 1622403420, 1622407800, 1622407860, 1622409600, 1622412780,\n",
       "         1622412960, 1622415060, 1622415240, 1622416500, 1622417400, 1622417580,\n",
       "         1622418300, 1622418720, 1622420340, 1622426820, 1622426940, 1622428020,\n",
       "         1622428440, 1622429220, 1622429340, 1622430300, 1622432400, 1622436060,\n",
       "         1622437320, 1622461140, 1622461500, 1622466420, 1622466780, 1622568960,\n",
       "         1622573880, 1622574300, 1622574420, 1622577480, 1622578020, 1622578560,\n",
       "         1622579040, 1622579220, 1622579820, 1622580060, 1622580600, 1622583900,\n",
       "         1622584380, 1622588040, 1622588040, 1622588340, 1622593440, 1622596620,\n",
       "         1622600160, 1622600280, 1622601000, 1622604900, 1622605320, 1622608260,\n",
       "         1622608380, 1622609220, 1622611790, 1622631360, 1622633760, 1622634660,\n",
       "         1622645580, 1622652960, 1622657280, 1622657940, 1622663280, 1622664248,\n",
       "         1622670120, 1622675220, 1622680020, 1622682300, 1622683920, 1622684340,\n",
       "         1622684760, 1622687100, 1622688120, 1622688360, 1622690460, 1622690460,\n",
       "         1622692860, 1622694540, 1622696100, 1622698380, 1622698800, 1622699820,\n",
       "         1622702520, 1622704620, 1622713620, 1622718420, 1622724120, 1622726880,\n",
       "         1622736840, 1622755140, 1622765427, 1622771400, 1622773920, 1622773926,\n",
       "         1622774100, 1622777280, 1622777520, 1622778660, 1622817480, 1622817540,\n",
       "         1622821380, 1622830620, 1622864760, 1622876700, 1622884680, 1622891880,\n",
       "         1622915580, 1622918820, 1622919120, 1622920320, 1622920380, 1622920680,\n",
       "         1622921880, 1622922060, 1622922180, 1622923140, 1622924460, 1622925060,\n",
       "         1622925900, 1622926200, 1622926380, 1622926380, 1622927040, 1622928120,\n",
       "         1622928660, 1622928719, 1622962140, 1623106336, 1623237895, 1623477780,\n",
       "         1623561600, 1623565320, 1623567120, 1623582840, 1623660720, 1623776760,\n",
       "         1623815146, 1623850200, 1623853200, 1623856920, 1623877320, 1623898260,\n",
       "         1623898920, 1623899880, 1623903360, 1624131396, 1624216860, 1624566120,\n",
       "         1624566300, 1624568160, 1624571160, 1624571580, 1624571640, 1624573320,\n",
       "         1624582500, 1624585560, 1624586160, 1624587480, 1624588320, 1624589220,\n",
       "         1624589220, 1624590480, 1624591380, 1624593480, 1624595220, 1624595580,\n",
       "         1624597375, 1624600980, 1624610580, 1624814009, 1624872360, 1625106240,\n",
       "         1625110860, 1625113320, 1625115960, 1625167440, 1625167500, 1625168040,\n",
       "         1625191140, 1625194560, 1625202180, 1625203440, 1625209920, 1625224860,\n",
       "         1625250960, 1625271720, 1625272800, 1625282940, 1625291400, 1625292060,\n",
       "         1625293080, 1625293260, 1625296020, 1625298480, 1625298720, 1625300760,\n",
       "         1625302140, 1625303820, 1625304360, 1625305020, 1625305740, 1625307000,\n",
       "         1625307900, 1625308500, 1625308740, 1625309100, 1625311080, 1625313480,\n",
       "         1625377920, 1625379660, 1625380800, 1625383440, 1625443020, 1625450880,\n",
       "         1625471460, 1625486640, 1625502240, 1625507100, 1625509080, 1625806298,\n",
       "         1625986380, 1625989920, 1625991420, 1625993400, 1626114480, 1626115605,\n",
       "         1626155640, 1626199500, 1626200580, 1626200820, 1626202320, 1626205080,\n",
       "         1626205980, 1626206040, 1626207960, 1626209460, 1626218460, 1626225660,\n",
       "         1626226020, 1626228540, 1626230640, 1626241080, 1626248340, 1626265020,\n",
       "         1626281640, 1626416817, 1626458580, 1626466440, 1626467400, 1626470580,\n",
       "         1626472260, 1626474540, 1626494280, 1626497040, 1626497640, 1626498900,\n",
       "         1626500520, 1626503520, 1626503520, 1626507960, 1626511980, 1626516900,\n",
       "         1626522960, 1626527400, 1626832140, 1626835080, 1626838560, 1626838920,\n",
       "         1626838980, 1626840300, 1626843660, 1626848340, 1626850320, 1626866640,\n",
       "         1626896460, 1626897120, 1626951060])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ptls.data_load.datasets import ParquetDataset, ParquetFiles\n",
    "\n",
    "iterable_train = ParquetDataset(ParquetFiles('train.parquet'))\n",
    "next(iter(iterable_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter, FeatureFilter\n",
    "\n",
    "map_processed_train = MemoryMapDataset(\n",
    "    data=iterable_train,\n",
    "    i_filters=[\n",
    "        SeqLenFilter(min_seq_len=25),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '35105d846c1e41b0bebc0515ddfc31e9',\n",
       " 'cat_id': tensor([32, 19, 37, 37, 12, 15, 15,  4,  4,  4, 11,  4, 11,  4, 11,  8, 53,  8,\n",
       "         11,  3,  1,  1, 45,  7, 53,  7,  7,  7,  7,  7,  7, 40, 40,  3,  8, 40,\n",
       "         40,  4,  1,  3,  3,  3,  1,  1,  1,  7,  3,  4,  1, 25,  1,  1, 25,  1,\n",
       "          1,  1,  1,  3,  3,  8,  1,  8, 37,  1,  8,  8,  5,  3,  8,  1,  4,  7,\n",
       "         37, 45, 45, 45, 15, 15, 45, 15, 45, 45, 32, 15, 15,  1, 45, 15, 45, 45,\n",
       "         15, 15, 15, 15, 45, 15, 45, 15, 12, 15, 15, 45, 45, 45, 45, 15, 45, 12,\n",
       "         15, 13, 15, 15, 45, 15, 45, 45, 15, 45,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "         12, 15,  3,  1, 45, 15, 15, 45, 45,  1,  1, 20, 20,  1,  3, 45,  4,  4,\n",
       "          7,  7,  7,  7,  4,  7, 49,  4,  7,  7, 49,  7,  7,  7,  4,  1,  7,  7,\n",
       "          7,  7,  7,  7,  7,  4,  7,  7,  7,  7,  4,  7,  7,  7,  7, 21,  7,  1,\n",
       "          1,  1,  1,  7,  7,  1,  7,  1,  7,  7, 14, 49, 11,  4,  4,  7,  3, 21,\n",
       "         15, 45, 45, 45, 45, 45, 45, 45, 45,  1, 45, 45, 45, 45, 45,  1,  1, 45,\n",
       "          1, 45,  1, 45, 45, 45, 15, 15,  7,  7,  7,  7,  7,  1,  4,  7,  7,  7,\n",
       "          1,  1,  1,  1,  3,  4,  4,  3,  1,  7,  4,  4,  8,  7,  4,  4,  4,  4,\n",
       "          8,  8,  4,  4,  4,  7,  7, 15,  1,  3,  2, 11,  2,  1,  3,  4,  4,  4,\n",
       "         11,  7,  1,  7,  7,  7,  1, 31,  1,  1, 45, 45, 45, 45,  3, 45, 45,  3,\n",
       "          4,  8,  8, 11,  4,  4,  4,  3,  3, 12,  8,  4,  3,  1,  4,  7,  7,  4,\n",
       "          4,  4,  4,  4,  4,  7,  4,  3,  4,  3, 21,  4,  4,  4,  1,  1,  1, 14,\n",
       "          3,  3,  1,  7,  1,  1,  7,  3,  3,  7,  7,  3,  1,  8, 45, 45, 32, 45,\n",
       "         45, 45, 45,  1,  1, 12,  4,  1, 15, 11,  4, 31, 31,  1, 31,  8, 31,  1,\n",
       "          7,  7,  8,  7,  7, 11, 11,  7, 14,  3, 14, 45, 45, 15, 45, 11,  8, 31,\n",
       "         31,  3,  3,  1,  1, 21,  8,  4,  4, 39,  4,  3, 31, 31,  1, 12, 12, 31,\n",
       "         14, 31,  8,  4,  8,  1,  1,  3,  1,  8, 45, 15, 45, 31, 21, 45,  1, 45,\n",
       "         31,  1,  4,  4,  8, 31, 31,  3,  1, 31,  4,  8, 45,  1, 31, 45, 31, 31,\n",
       "         31, 31, 31,  1,  4,  4,  4,  4,  1,  4,  4,  4,  4, 31, 40, 12,  3,  3,\n",
       "          3,  8, 11, 12, 45, 12, 12, 12, 12,  4, 46, 12,  7,  4, 12, 28,  8, 49,\n",
       "          8, 29, 24,  5,  5,  4,  1,  4,  4,  4,  4,  4, 11,  3,  4,  3,  1, 11,\n",
       "          4,  7,  7,  3, 49,  7,  7,  5,  7,  5,  8,  7,  1,  8,  7,  8,  7,  7,\n",
       "         11,  7,  3,  8, 15,  1,  3,  3,  4,  4,  8,  4, 11,  4,  4,  4,  1,  4,\n",
       "          4,  4,  4,  4,  4, 14,  1,  7,  8,  3,  4, 11,  4, 37, 37,  1,  3, 36,\n",
       "         46, 28, 36,  8, 28,  3,  3, 46, 28,  1,  8, 28, 28, 28, 28, 28, 28, 28,\n",
       "          4,  4,  3,  4, 28, 28, 28, 28,  1,  1, 28, 28, 28, 36, 28, 28, 28, 19,\n",
       "         28, 28, 28, 11,  4, 11,  4,  4,  4,  4,  4, 28,  8, 12,  8,  5,  8, 15,\n",
       "          8,  1, 28,  8,  8, 28,  8, 23,  4,  7,  7,  8,  7,  7,  7,  7, 12, 12,\n",
       "         12, 12, 12, 12, 12, 12,  9, 37, 37, 40, 37, 37,  1, 37, 37, 40, 37,  1,\n",
       "         40, 37, 37,  1, 37,  3, 19,  3,  8, 19, 12, 12,  7,  7, 12, 12, 12, 15,\n",
       "         28, 14,  7,  7,  1,  1,  8,  7,  7, 15,  9, 45, 45, 45, 45, 15, 45, 15,\n",
       "          1,  3,  4,  3, 11, 11, 45,  3,  4, 11,  4,  1, 11,  1,  1, 31, 45,  9,\n",
       "          1,  1,  3,  1,  8,  1,  3,  3, 24, 24, 12,  8, 31, 31,  1, 31,  3,  1,\n",
       "          3,  3,  1,  1, 31, 23,  1,  8,  1, 11, 11,  1,  3, 20,  4, 11, 19,  8,\n",
       "         35,  7,  7,  7, 31, 31, 31,  8, 31, 31,  8,  1,  6, 45, 45, 45, 36, 31,\n",
       "         12,  4,  3, 31,  1,  3,  2,  4,  1, 31,  8, 31, 31, 31,  8, 31, 31, 31,\n",
       "         31,  7, 31,  1,  8,  8, 31, 31, 31,  8, 31,  1,  4,  4,  1, 31,  1, 12,\n",
       "         12,  1,  7,  7,  7, 31,  7,  7, 31, 31, 31, 31, 45,  1,  3],\n",
       "        dtype=torch.int32),\n",
       " 'event_time': tensor([1612149300, 1612258440, 1612258560, 1612262940, 1612357740, 1612373880,\n",
       "         1612374240, 1612375800, 1612377060, 1612382340, 1612405800, 1612405860,\n",
       "         1612406700, 1612408500, 1612409520, 1612411920, 1612415580, 1612427760,\n",
       "         1612489260, 1612497480, 1612497540, 1612497840, 1612546800, 1612592760,\n",
       "         1612663500, 1612829820, 1612917900, 1612919640, 1612919940, 1612921560,\n",
       "         1612926060, 1613038680, 1613039580, 1613042580, 1613043960, 1613045040,\n",
       "         1613045340, 1613045760, 1613049120, 1613233440, 1613235420, 1613236140,\n",
       "         1613236980, 1613240280, 1613243580, 1613277000, 1613279400, 1613279700,\n",
       "         1613282040, 1613285400, 1613286000, 1613288700, 1613289060, 1613290800,\n",
       "         1613291580, 1613294160, 1613297940, 1613306100, 1613310120, 1613310600,\n",
       "         1613311680, 1613327580, 1613329260, 1613345760, 1613360160, 1613361780,\n",
       "         1613388600, 1613395140, 1613396520, 1613397060, 1613398500, 1613523657,\n",
       "         1614535677, 1614844320, 1614844380, 1614846000, 1614850140, 1614851280,\n",
       "         1614873000, 1614879900, 1615180637, 1615261980, 1615262220, 1615262340,\n",
       "         1615262460, 1615262760, 1615262760, 1615262760, 1615262820, 1615263060,\n",
       "         1615263060, 1615264140, 1615264320, 1615264440, 1615265280, 1615266780,\n",
       "         1615266900, 1615266960, 1615267020, 1615267200, 1615267260, 1615267500,\n",
       "         1615268040, 1615268460, 1615268820, 1615269000, 1615269120, 1615269274,\n",
       "         1615269360, 1615269480, 1615269540, 1615269960, 1615269960, 1615270080,\n",
       "         1615270260, 1615270260, 1615270380, 1615270680, 1616029920, 1616031300,\n",
       "         1616033580, 1616036280, 1616036460, 1616036760, 1616037660, 1616037720,\n",
       "         1616080200, 1616140200, 1616259000, 1616262600, 1616264580, 1616265480,\n",
       "         1616266080, 1616267100, 1616267220, 1616295540, 1616295660, 1616300220,\n",
       "         1616300280, 1616325480, 1616337480, 1616342532, 1616460480, 1616467680,\n",
       "         1616505780, 1616510220, 1616510220, 1616510880, 1616515140, 1616515440,\n",
       "         1616518440, 1616521560, 1616550720, 1616550720, 1616551380, 1616551380,\n",
       "         1616551440, 1616551560, 1616551680, 1616551980, 1616552280, 1616552820,\n",
       "         1616553120, 1616554620, 1616555640, 1616556240, 1616556660, 1616557080,\n",
       "         1616557500, 1616557680, 1616557740, 1616557980, 1616558640, 1616558700,\n",
       "         1616558700, 1616560140, 1616561040, 1616562120, 1616562300, 1616609774,\n",
       "         1616786820, 1616830800, 1616845920, 1616845920, 1616846880, 1616848080,\n",
       "         1616851680, 1616924400, 1617237960, 1617238620, 1617240240, 1617240660,\n",
       "         1617241680, 1617244200, 1617244800, 1617246300, 1617252896, 1617254040,\n",
       "         1617277800, 1617281160, 1617284520, 1617382860, 1617382920, 1617383220,\n",
       "         1617384120, 1617385500, 1617387420, 1617387540, 1617388020, 1617388800,\n",
       "         1617389040, 1617390300, 1617390840, 1617391260, 1617391440, 1617392040,\n",
       "         1617467520, 1617641700, 1617645000, 1617646440, 1617646680, 1617646920,\n",
       "         1617648600, 1617650160, 1617670920, 1617672300, 1617672600, 1617673800,\n",
       "         1617674040, 1617675059, 1617676080, 1617676440, 1617679380, 1617769402,\n",
       "         1617889581, 1618077840, 1618080360, 1618083300, 1618104924, 1618114680,\n",
       "         1618117740, 1618118460, 1618118760, 1618121340, 1618123320, 1618123680,\n",
       "         1618126200, 1618126980, 1618127040, 1618129080, 1618130700, 1618132380,\n",
       "         1618156560, 1618161720, 1618194960, 1618200240, 1618277520, 1618280640,\n",
       "         1618282860, 1618287240, 1618287240, 1618330800, 1618331940, 1618452000,\n",
       "         1618457100, 1618459380, 1618460580, 1618462200, 1618463460, 1618464060,\n",
       "         1618495500, 1618495680, 1618497780, 1618497840, 1618500420, 1618502940,\n",
       "         1618542900, 1618544579, 1618544640, 1618562760, 1618566780, 1618567020,\n",
       "         1618568880, 1618571880, 1618574149, 1618574580, 1618575720, 1618590720,\n",
       "         1618626507, 1618723320, 1618724100, 1618728720, 1618731660, 1618732140,\n",
       "         1618734420, 1618746360, 1618747680, 1618749660, 1618752420, 1618753560,\n",
       "         1618755000, 1618776900, 1618797120, 1618797600, 1618798860, 1618799940,\n",
       "         1618802220, 1618802340, 1618802940, 1618803300, 1618804800, 1618805520,\n",
       "         1618807860, 1618808520, 1618809240, 1618832940, 1618833060, 1618836360,\n",
       "         1618839300, 1618839660, 1618840440, 1618842660, 1618843560, 1618850760,\n",
       "         1618958820, 1618959300, 1618960140, 1618960860, 1618961340, 1618962060,\n",
       "         1618964160, 1618964400, 1618964640, 1618965480, 1618965780, 1619013720,\n",
       "         1619014440, 1619107504, 1619113620, 1619114640, 1619115000, 1619118000,\n",
       "         1619120280, 1619150220, 1619151480, 1619159580, 1619168040, 1619215020,\n",
       "         1619249940, 1619252940, 1619272140, 1619366189, 1619412850, 1619470140,\n",
       "         1619470740, 1619484360, 1619491860, 1619504820, 1619506860, 1619523960,\n",
       "         1619528040, 1619529960, 1619530680, 1619533140, 1619533140, 1619535720,\n",
       "         1619536200, 1619536680, 1619536980, 1619539500, 1619542140, 1619545680,\n",
       "         1619548140, 1619548860, 1619549160, 1619577300, 1619578620, 1619580060,\n",
       "         1619588100, 1619607960, 1619609100, 1619609460, 1619610540, 1619610540,\n",
       "         1619615880, 1619619600, 1619620920, 1619624520, 1619624700, 1619626740,\n",
       "         1619648580, 1619651220, 1619651460, 1619652420, 1619653200, 1619657340,\n",
       "         1619664420, 1619681100, 1619697240, 1619704560, 1619716620, 1619803680,\n",
       "         1619805120, 1619809500, 1619835825, 1619928933, 1619962620, 1619963640,\n",
       "         1619965860, 1620014683, 1620048767, 1620159107, 1620580020, 1620580260,\n",
       "         1620600000, 1620618660, 1620620400, 1620620460, 1620624180, 1620627120,\n",
       "         1620633240, 1620633840, 1620651780, 1620654780, 1620657540, 1620660060,\n",
       "         1620661560, 1620663000, 1620668940, 1620672676, 1620726780, 1620766920,\n",
       "         1620770040, 1620784860, 1620791700, 1620798720, 1620798900, 1620801120,\n",
       "         1620801600, 1620801600, 1620801900, 1620802440, 1620803700, 1620806100,\n",
       "         1620807060, 1620810420, 1620814620, 1620818580, 1620827940, 1620833160,\n",
       "         1620834180, 1620834180, 1620835680, 1620994860, 1621006532, 1621055160,\n",
       "         1621060740, 1621089360, 1621340700, 1621501086, 1621541882, 1621622760,\n",
       "         1621649671, 1621846973, 1621951764, 1621998409, 1622092020, 1622097720,\n",
       "         1622098440, 1622098680, 1622226960, 1622227260, 1622232600, 1622257980,\n",
       "         1622259780, 1622260140, 1622260860, 1622261220, 1622261760, 1622262000,\n",
       "         1622262900, 1622263860, 1622264040, 1622266680, 1622266920, 1622267760,\n",
       "         1622268300, 1622268900, 1622269380, 1622269980, 1622270100, 1622270880,\n",
       "         1622270880, 1622271780, 1622272175, 1622272200, 1622272380, 1622272500,\n",
       "         1622272800, 1622273760, 1622273880, 1622273880, 1622274120, 1622274960,\n",
       "         1622276280, 1622276400, 1622280660, 1622281080, 1622281440, 1622290320,\n",
       "         1622401200, 1622403420, 1622407800, 1622407860, 1622409600, 1622412780,\n",
       "         1622412960, 1622415060, 1622415240, 1622416500, 1622417400, 1622417580,\n",
       "         1622418300, 1622418720, 1622420340, 1622426820, 1622426940, 1622428020,\n",
       "         1622428440, 1622429220, 1622429340, 1622430300, 1622432400, 1622436060,\n",
       "         1622437320, 1622461140, 1622461500, 1622466420, 1622466780, 1622568960,\n",
       "         1622573880, 1622574300, 1622574420, 1622577480, 1622578020, 1622578560,\n",
       "         1622579040, 1622579220, 1622579820, 1622580060, 1622580600, 1622583900,\n",
       "         1622584380, 1622588040, 1622588040, 1622588340, 1622593440, 1622596620,\n",
       "         1622600160, 1622600280, 1622601000, 1622604900, 1622605320, 1622608260,\n",
       "         1622608380, 1622609220, 1622611790, 1622631360, 1622633760, 1622634660,\n",
       "         1622645580, 1622652960, 1622657280, 1622657940, 1622663280, 1622664248,\n",
       "         1622670120, 1622675220, 1622680020, 1622682300, 1622683920, 1622684340,\n",
       "         1622684760, 1622687100, 1622688120, 1622688360, 1622690460, 1622690460,\n",
       "         1622692860, 1622694540, 1622696100, 1622698380, 1622698800, 1622699820,\n",
       "         1622702520, 1622704620, 1622713620, 1622718420, 1622724120, 1622726880,\n",
       "         1622736840, 1622755140, 1622765427, 1622771400, 1622773920, 1622773926,\n",
       "         1622774100, 1622777280, 1622777520, 1622778660, 1622817480, 1622817540,\n",
       "         1622821380, 1622830620, 1622864760, 1622876700, 1622884680, 1622891880,\n",
       "         1622915580, 1622918820, 1622919120, 1622920320, 1622920380, 1622920680,\n",
       "         1622921880, 1622922060, 1622922180, 1622923140, 1622924460, 1622925060,\n",
       "         1622925900, 1622926200, 1622926380, 1622926380, 1622927040, 1622928120,\n",
       "         1622928660, 1622928719, 1622962140, 1623106336, 1623237895, 1623477780,\n",
       "         1623561600, 1623565320, 1623567120, 1623582840, 1623660720, 1623776760,\n",
       "         1623815146, 1623850200, 1623853200, 1623856920, 1623877320, 1623898260,\n",
       "         1623898920, 1623899880, 1623903360, 1624131396, 1624216860, 1624566120,\n",
       "         1624566300, 1624568160, 1624571160, 1624571580, 1624571640, 1624573320,\n",
       "         1624582500, 1624585560, 1624586160, 1624587480, 1624588320, 1624589220,\n",
       "         1624589220, 1624590480, 1624591380, 1624593480, 1624595220, 1624595580,\n",
       "         1624597375, 1624600980, 1624610580, 1624814009, 1624872360, 1625106240,\n",
       "         1625110860, 1625113320, 1625115960, 1625167440, 1625167500, 1625168040,\n",
       "         1625191140, 1625194560, 1625202180, 1625203440, 1625209920, 1625224860,\n",
       "         1625250960, 1625271720, 1625272800, 1625282940, 1625291400, 1625292060,\n",
       "         1625293080, 1625293260, 1625296020, 1625298480, 1625298720, 1625300760,\n",
       "         1625302140, 1625303820, 1625304360, 1625305020, 1625305740, 1625307000,\n",
       "         1625307900, 1625308500, 1625308740, 1625309100, 1625311080, 1625313480,\n",
       "         1625377920, 1625379660, 1625380800, 1625383440, 1625443020, 1625450880,\n",
       "         1625471460, 1625486640, 1625502240, 1625507100, 1625509080, 1625806298,\n",
       "         1625986380, 1625989920, 1625991420, 1625993400, 1626114480, 1626115605,\n",
       "         1626155640, 1626199500, 1626200580, 1626200820, 1626202320, 1626205080,\n",
       "         1626205980, 1626206040, 1626207960, 1626209460, 1626218460, 1626225660,\n",
       "         1626226020, 1626228540, 1626230640, 1626241080, 1626248340, 1626265020,\n",
       "         1626281640, 1626416817, 1626458580, 1626466440, 1626467400, 1626470580,\n",
       "         1626472260, 1626474540, 1626494280, 1626497040, 1626497640, 1626498900,\n",
       "         1626500520, 1626503520, 1626503520, 1626507960, 1626511980, 1626516900,\n",
       "         1626522960, 1626527400, 1626832140, 1626835080, 1626838560, 1626838920,\n",
       "         1626838980, 1626840300, 1626843660, 1626848340, 1626850320, 1626866640,\n",
       "         1626896460, 1626897120, 1626951060])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_processed_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from ptls.nn import TrxEncoder, RnnSeqEncoder\n",
    "from ptls.frames.coles import CoLESModule\n",
    "\n",
    "trx_encoder_params = dict(\n",
    "    embeddings_noise=0.003,\n",
    "    numeric_values={},\n",
    "    embeddings={\n",
    "        'event_time': {'in': 800, 'out': 16},\n",
    "        'cat_id': {'in': 410, 'out': 16},\n",
    "    },\n",
    ")\n",
    "\n",
    "seq_encoder = RnnSeqEncoder(\n",
    "    trx_encoder=TrxEncoder(**trx_encoder_params),\n",
    "    hidden_size=256,\n",
    "    type='gru',\n",
    ")\n",
    "\n",
    "model = CoLESModule(\n",
    "    seq_encoder=seq_encoder,\n",
    "    optimizer_partial=partial(torch.optim.Adam, lr=0.001),\n",
    "    lr_scheduler_partial=partial(torch.optim.lr_scheduler.StepLR, step_size=30, gamma=0.9),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import MemoryMapDataset\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.frames.coles import ColesDataset\n",
    "from ptls.frames.coles.split_strategy import SampleSlices\n",
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "train_dl = PtlsDataModule(\n",
    "    train_data=ColesDataset(\n",
    "        map_processed_train,\n",
    "        splitter=SampleSlices(\n",
    "            split_count=5,\n",
    "            cnt_min=25,\n",
    "            cnt_max=200,\n",
    "        ),\n",
    "    ),\n",
    "    train_num_workers=16,\n",
    "    train_batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import logging\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=15,\n",
    "    gpus=1 if torch.cuda.is_available() else 0,\n",
    "    enable_progress_bar=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type            | Params\n",
      "-------------------------------------------------------\n",
      "0 | _loss              | ContrastiveLoss | 0     \n",
      "1 | _seq_encoder       | RnnSeqEncoder   | 242 K \n",
      "2 | _validation_metric | BatchRecallTopK | 0     \n",
      "3 | _head              | Head            | 0     \n",
      "-------------------------------------------------------\n",
      "242 K     Trainable params\n",
      "0         Non-trainable params\n",
      "242 K     Total params\n",
      "0.969     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logger.version = 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 0 bytes free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:770\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    753\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    769\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 770\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    771\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    772\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:723\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    722\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 723\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    724\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:811\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    807\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    808\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    809\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    810\u001b[0m )\n\u001b[1;32m--> 811\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    813\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    814\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1236\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1236\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1238\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1239\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1323\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1321\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1323\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1353\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1351\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1352\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:266\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    263\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    264\u001b[0m )\n\u001b[0;32m    265\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 266\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    207\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    212\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     87\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[0;32m    204\u001b[0m         batch,\n\u001b[0;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[0;32m    206\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[0;32m    207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    211\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    249\u001b[0m         closure()\n\u001b[0;32m    251\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[0;32m    258\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    368\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    370\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    372\u001b[0m     batch_idx,\n\u001b[0;32m    373\u001b[0m     optimizer,\n\u001b[0;32m    374\u001b[0m     opt_idx,\n\u001b[0;32m    375\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    376\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[0;32m    377\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[0;32m    378\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1595\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1592\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1594\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1595\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1597\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1598\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1646\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1565\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1566\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1573\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1574\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1575\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1576\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1644\u001b[0m \n\u001b[0;32m   1645\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1646\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:155\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m    154\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:23\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 23\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     24\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py:183\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 183\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    185\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    186\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:140\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    134\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:143\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_fn()\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[0;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:311\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer, opt_idx)\n\u001b[0;32m    313\u001b[0m     \u001b[39m# check if model weights are nan\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_terminate_on_nan:\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1765\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1762\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1764\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1765\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1767\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1768\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:168\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[1;34m(self, closure_loss, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_backward(closure_loss)\n\u001b[0;32m    166\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n\u001b[1;32m--> 168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mbackward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:80\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[1;34m(self, model, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m# do backward pass\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m---> 80\u001b[0m     model\u001b[39m.\u001b[39mbackward(closure_loss, optimizer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_backward(closure_loss, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1391\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[1;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\n\u001b[0;32m   1375\u001b[0m     \u001b[39mself\u001b[39m, loss: Tensor, optimizer: Optional[Optimizer], optimizer_idx: Optional[\u001b[39mint\u001b[39m], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   1376\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[39m\"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your\u001b[39;00m\n\u001b[0;32m   1378\u001b[0m \u001b[39m    own implementation if you need to.\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1389\u001b[0m \u001b[39m            loss.backward()\u001b[39;00m\n\u001b[0;32m   1390\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1391\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 764.00 MiB (GPU 0; 4.00 GiB total capacity; 2.31 GiB already allocated; 0 bytes free; 3.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(f'logger.version = {trainer.logger.version}')\n",
    "trainer.fit(model, train_dl)\n",
    "print(trainer.logged_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(seq_encoder.state_dict(), \"coles-emb-clickstream.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.data_load.datasets import inference_data_loader\n",
    "\n",
    "iterable_test = ParquetDataset(ParquetFiles('test.parquet'))\n",
    "\n",
    "train_dl = inference_data_loader(iterable_train, num_workers=0, batch_size=256)\n",
    "train_embeds = torch.vstack(trainer.predict(model, train_dl, ))\n",
    "\n",
    "test_dl = inference_data_loader(iterable_test, num_workers=0, batch_size=256)\n",
    "test_embeds = torch.vstack(trainer.predict(model, test_dl))\n",
    "\n",
    "train_embeds.shape, test_embeds.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
